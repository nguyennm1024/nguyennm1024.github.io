<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Nguyen (Will) Nguyen</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" href=timeline.css>

  <link rel="icon" type="image/png" href="images/buffalo_icon.png">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Nguyen (William) Nguyen</name>
                  </p>
                  <p>Currently, I am a Senior Applied Scientist at <a href="https://www.aitomatic.com">Aitomatic</a>. I
                    graduated with a master's degree from the University of Rochester, where I was advised by Professor
                    <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>. During my time at the University of
                    Rochester, I worked on instructional video understanding and vision-language problems.
                  </p>
                  <p>I had nearly 3 wonderful years at <a href="https://www.vinai.io/">VinAI Research</a> where I worked
                    on the scene text spotting problem under the supervision of Professor <a
                      href="https://www3.cs.stonybrook.edu/~minhhoai/">Nguyen Minh Hoai</a>. I completed my bachelor's
                    degree at the <a href="https://e.uet.vnu.edu.vn/">University of Engineering and Technology -
                      VNU</a>, where I had the opportunity to work with Professor <a
                      href="https://sites.google.com/site/xiemhoang/">Hoang Van Xiem</a>.</p>
                  <p style="text-align:center">
                    <a href="mailto:nguyennm1024@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="data/CV_Nguyen-2.pdf">CV</a> &nbsp/&nbsp
                    <a href="data/Nguyen-bio.txt">Bio</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=kYok1lsAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://twitter.com/nguyennm1024">Twitter</a> &nbsp/&nbsp
                    <a href="https://github.com/nguyennm1024">Github</a> <br>
                    <a href="https://chatgpt.com/g/g-67d35695596481919e9deb894d707a24-william-nguyen"
                      class="twin-ai-button" target="_blank">✨ Chat with my AI Twin ✨</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/ManhNguyen.png"><img style="width:70%;max-width:70%" alt="profile photo"
                      src="images/ManhNguyen_circle.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <div style="width:100%; margin-right:auto; margin-left:auto;">
            <div style="padding:20px; vertical-align:middle;">
              <heading>News</heading>
              <div style="height:200px; overflow:auto;">
                <ul>
                  <li>03/2025: One paper accepted at JSAI 2025.</li> <br>
                  <li>12/2024: One paper accepted at OSAI4MU-25 Workshop, AAAI 2025.</li> <br>
                  <li>07/2024: Our SemiKong work was featured in <a
                      href="https://venturebeat.com/ai/aitomatics-semikong-uses-ai-to-reshape-chipmaking-processes/">VentureBeat</a>,
                    <a
                      href="https://www.msn.com/en-us/money/other/semikong-is-the-worlds-first-open-source-semiconductor-focused-llm-it-claims-to-bring-new-chips-to-market-30-faster/ar-AA1wD6nS">MSN</a>,
                    <a
                      href="https://www.linkedin.com/posts/yann-lecun_semikong-by-aitomatic-built-with-llama-to-activity-7270625478237945856-cTex/">Yann
                      LeCun's share</a>,
                    <a href="https://ai.meta.com/blog/aitomatic-built-with-llama/">Meta AI Blog</a>,
                    <a
                      href="https://www.tomshardware.com/tech-industry/artificial-intelligence/semikong-is-the-worlds-first-open-source-semiconductor-focused-llm-it-claims-to-bring-new-chips-to-market-30-percent-faster">Tom's
                      Hardware</a>,
                    <a
                      href="https://www.marktechpost.com/2024/12/27/meet-semikong-the-worlds-first-open-source-semiconductor-focused-llm/">MarkTechPost</a>,
                    <a
                      href="https://digialps.com/semikong-an-open-source-semiconductor-centric-llm-powered-by-meta-llama-3-1/">Digialps</a>,
                    <a
                      href="https://www.gadgets360.com/ai/news/aitomatic-ai-alliance-semikong-model-semiconductor-focused-open-source-meta-7363931">Gadgets360</a>,
                    and many other media.


                  </li> <br>
                  <li>07/2024: One paper accepted at ACM MM 2024.</li> <br>
                  <li>07/2024: I joined Aitomatic as a Senior Applied Scientist.</li> <br>
                  <li>03/2024: One paper accepted at NAACL 2024.</li> <br>
                  <li>03/2024: I received research internship offers from Bosch AI Research and Amazon, USA.</li> <br>
                  <li>07/2023: One paper accepted at AV4D Workshop, ICCV 2023.</li> <br>
                  <li>08/2022: I started my journey with the University of Rochester since the Fall 2022.</li> <br>
                  <li>01/2022: I began working as an AI Research Engineer with the applied team at VinAI Research.</li>
                  <br>
                  <li>03/2021: One paper accepted at CVPR 2021.</li> <br>
                  <li>07/2020: I graduated with Distinction from Vietnam National University in 2020.</li> <br>
                  <li>12/2019: I started my journey with VinAI Research as an AI Research Resident in December 2019.
                  </li> <br>
                </ul>
              </div>
            </div>
          </div>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    I'm interested in computer vision, natural language processing, and machine learning (especially
                    deep learning). Much of my research lies in optical character recognition, focusing on scene text
                    recognition. I am also fond of Vision-Language problems, such as understanding visual content using
                    natural language. Recently, I am working on developing domain-specific foundation models for
                    industry.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-right:auto; margin-left:auto;">
            <tbody>

              <td style="padding:20px; width:25%; vertical-align:middle;">
                <img src='images/llamarine.png' width="160">

              </td>

              <td style="padding:20px; width:75%; vertical-align:middle;">
                <a href="">
                  <papertitle>Llamarine: Open-source Maritime Industry-specific Large Language Model</papertitle>
                </a>
                <br>
                <strong>William Nguyen</strong>,
                <a href="">An Phan</a>,
                <a href="">Konobu Kimura</a>,
                <a href="">Hitoshi Maeno</a>,
                <a href="">Mika Tanaka</a>,
                <a href="">Quynh Le</a>,
                <a href="">William Poucher</a>,
                <a href="https://scholar.google.com/citations?user=3KvkkfoAAAAJ&hl=en">Christopher Nguyen</a>,
                <br>
                <em>preprint</em>, 2025
                <br>
                <a href="https://arxiv.org/abs/2503.00203">paper</a>
                <p style="line-height:1.5;">
                  The first opensource domain-specific LLM for the maritime industry. Our model outperforms
                  serveral commercial products, including GPT-4o-mini GPT-4o, Claude-3.5-Sonnet, and opensource
                  models such as Llama3.1 8B, Llama3.1 70B, and Llama3.3 70B.
                </p>
              </td>
      </tr>
    </tbody>
  </table>

  <table
    style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-right:auto; margin-left:auto;">
    <tbody>

      <td style="padding:20px; width:25%; vertical-align:middle;">
        <img src='images/semikong.png' width="160">

      </td>

      <td style="padding:20px; width:75%; vertical-align:middle;">
        <a href="">
          <papertitle>SemiKong: Curating, Training, and Evaluating A Semiconductor Industry-Specific Large
            Language Model</papertitle>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?user=3KvkkfoAAAAJ&hl=en">Christopher Nguyen</a>,
        <strong>William Nguyen</strong>,
        <a href="">Atsushi Suzuki</a>,
        <a href="">Daisuke Oku</a>,
        <a href="">Hong An Phan</a>,
        <a href="">Sang Dinh</a>,
        <a href="">Zooey Nguyen</a>,
        <a href="">Anh Hai Ha</a>,
        <a href="https://www.linkedin.com/in/shrutiraghavan/">Shruti Raghavan</a>,
        <a href="">Huy Vo</a>,
        <a href="">Thang Nguyen</a>,
        <a href="">Lan Nguyen</a>,
        <a href="">Yoshikuni Hirayama</a>
        <br>
        <em>OSAI4MU-25 Workshop, AAAI</em>, 2025
        <br>
        <a href="">project page</a> /
        <a href="">github</a> /
        <a href="https://arxiv.org/abs/2411.13802">paper</a>
        <p style="line-height:1.5;">
          The first opensource domain-specific LLM for the semiconductor industry. Our model outperforms
          serveral commercial products, including Claude-3.5-Sonnet, Haiku, Opus, Command-R, and opensource
          models such as Llama3 70B.
        </p>
      </td>
      </tr>
    </tbody>
  </table>

  <table
    style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-right:auto; margin-left:auto;">
    <tbody>

      <td style="padding:20px; width:25%; vertical-align:middle;">
        <img src='images/dana.png' width="160">

      </td>

      <td style="padding:20px; width:75%; vertical-align:middle;">
        <a href="">
          <papertitle>DANA: Domain-Aware Neurosymbolic Agents for Consistency and Accuracy</papertitle>
        </a>
        <br>
        <a href="https://www.linkedin.com/in/luongthevinh/">Vinh Luong</a>,
        <a href="">Sang Dinh</a>,
        <a href="https://www.linkedin.com/in/shrutiraghavan/">Shruti Raghavan</a>,
        <strong>William Nguyen</strong>,
        <a href="">Zooey Nguyen</a>,
        <a href="">Quynh Le</a>,
        <a href="">Hung Vo</a>,
        <a href="">Kentaro Maegaito</a>,
        <a href="">Loc Nguyen</a>,
        <a href="">Thao Nguyen</a>,
        <a href="">Anh Hai Ha</a>,
        <a href="https://scholar.google.com/citations?user=3KvkkfoAAAAJ&hl=en">Christopher Nguyen</a>,
        <br>
        <em>preprint</em>, 2024
        <br>
        <a href="https://thealliance.ai/blog/domain-aware-neurosymbolic-agent-dana-architecture">project page</a> /
        <a href="https://github.com/aitomatic/openssa/">github</a> /
        <a href="https://arxiv.org/abs/2410.02823">paper</a>
        <p style="line-height:1.5;">
          An unified agentic framework to incorporate expert knowledge using neurosymbolic to create domain specific
          agents that can perform with high consistency and accuracy, significantly outperform ChatGPT assistant and
          ReAct agent.
        </p>
      </td>
      </tr>
    </tbody>
  </table>



  <table
    style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-right:auto; margin-left:auto;">
    <tbody>

      <td style="padding:20px; width:25%; vertical-align:middle;">
        <img src='images/ea.png' width="160">

      </td>

      <td style="padding:20px; width:75%; vertical-align:middle;">
        <a href="">
          <papertitle>EAGLE: Egocentric AGgregated Language-video Engine</papertitle>
        </a>
        <br>
        <a href="https://jing-bi.github.io">Jing Bi</a>,
        <a href="http://yunlong10.github.io">Yunlong Tang</a>,
        <a href="https://songluchuan.github.io">Luchuan Song</a>,
        <a href="https://alivosoughi.com">Ali Vosoughi</a>,
        <strong>Nguyen Nguyen</strong>,
        <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
        <br>
        <em>ACM MM</em>, 2024
        <br>
        <a href="https://dl.acm.org/doi/10.1145/3664647.3681618">paper</a>
        <p style="line-height:1.5;">
          An unified multimodal LLM designed for comprehensively solving all tasks related to egocentric video
          understanding.
        </p>
      </td>
      </tr>
    </tbody>
  </table>

  <table
    style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-right:auto; margin-left:auto;">
    <tbody>

      <td style="padding:20px; width:25%; vertical-align:middle;">
        <img src='images/oscar.png' width="160">

      </td>

      <td style="padding:20px; width:75%; vertical-align:middle;">
        <a href="">
          <papertitle>OSCaR: Object State Captioning and State Change Representation</papertitle>
        </a>
        <br>
        <strong>Nguyen Nguyen</strong>,
        <a href="https://jing-bi.github.io">Jing Bi</a>,
        <a href="https://alivosoughi.com">Ali Vosoughi</a>,
        <a href="https://www.yapengtian.com">Yapeng Tian</a>,
        <a href="https://pooyanfazli.com">Pooyan Fazli</a>,
        <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
        <br>
        <em>NAACL</em>, 2024
        <br>
        <a href="https://github.com/nguyennm1024/OSCaR">github</a> /
        <a href="https://arxiv.org/abs/2402.17128">paper</a>
        <p style="line-height:1.5;">
          A new task of understanding the state of objects and the progression of object state changes. Besides, we
          trained a Multimodal-LLM that significantly surpasses previous state-of-the-art models. Our model achieved 90%
          quality of GPT4-V on both GPT4 and human evaluations.
        </p>
      </td>
      </tr>
    </tbody>
  </table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>


      <!-- <tr onmouseout="npil_stop()" onmouseover="npil_start()"> -->
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">

          <img src='images/linguistics_detection.png' width="160">

          <script type="text/javascript">
            function npil_start() {
              document.getElementById('npil_image').style.opacity = "1";
            }

            function npil_stop() {
              document.getElementById('npil_image').style.opacity = "0";
            }
            npil_stop()
          </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="">
          <papertitle>Efficiently Leveraging Linguistics Knowledge for Scene Text Spotting</papertitle>
        </a>
        <br>

        <strong>Nguyen Nguyen</strong>,
        <a href="https://www.yapengtian.com">Yapeng Tian</a>,
        <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
        <br>
        <em>Arxiv</em>
        <br>

        <a href="https://arxiv.org/abs/2402.17134">paper</a>
        <p></p>
        <p>
          A simple but effective approach to incorporate language knowledge from large text corpus for
          improving both text detection and recognition.
        </p>
      </td>
      </tr>


    </tbody>
  </table>

  <table
    style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-right:auto; margin-left:auto;">
    <tbody>
      <td style="padding:20px; width:25%; vertical-align:middle;">
        <img src='images/misar.jpeg' width="160">

      </td>
      <td style="padding:20px; width:75%; vertical-align:middle;">
        <a href="">
          <papertitle>MISAR: A Multimodal Instructional System with Augmented Reality</papertitle>
        </a>
        <br>
        <a href="https://jing-bi.github.io">Jing Bi</a>*,
        <strong>Nguyen Nguyen*</strong>,
        <a href="https://alivosoughi.com">Ali Vosoughi</a>*,
        <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a> (* equal contribution)
        <br>
        <em>AV4D Workshop, ICCV</em>, 2023
        <br>
        <a href="https://github.com/nguyennm1024/misar">github</a> /
        <a href="https://av4d.org/papers/iccv23/p12.pdf">paper</a>
        <p style="line-height:1.5;">
          A comprehensive system designed to guide humans to work more efficiently and accurately. Our system leverages
          the power of LLMs to interpret and process information from visual, auditory, and contextual dimensions.
        </p>
      </td>
      </tr>
    </tbody>
  </table>


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>


      <tr onmouseout="npil_stop()" onmouseover="npil_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='npil_image'>
              <img src='images/st_after.png' width="160">
            </div>
            <img src='images/st_before.png' width="160">
          </div>
          <script type="text/javascript">
            function npil_start() {
              document.getElementById('npil_image').style.opacity = "1";
            }

            function npil_stop() {
              document.getElementById('npil_image').style.opacity = "0";
            }
            npil_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://www.vinai.io/publication-posts/dictionary-guided-scene-text-recognition">
            <papertitle>Dictionary-guided Scene Text Recognition</papertitle>
          </a>
          <br>

          <strong>Nguyen Nguyen</strong>,
          <a href="">Thu Nguyen</a>,
          <a href="https://www3.cs.stonybrook.edu/~tquangvinh/">Vinh Tran</a>,
          <a href="https://www.fit.hcmus.edu.vn/~tmtriet/">Minh Triet Tran</a>,
          <a href="https://scholar.google.com/citations?user=I8bNZakAAAAJ&hl=en">Thanh Duc Ngo</a>,
          <a href="https://ix.cs.uoregon.edu/~thien/">Thien Huu Nguyen</a>,
          <a href="https://www3.cs.stonybrook.edu/~minhhoai/">Minh Hoai Nguyen</a>
          <br>
          <em>CVPR</em>, 2021
          <br>
          <a href="https://www.vinai.io/publication-posts/dictionary-guided-scene-text-recognition">project
            page</a> /
          <a href="https://github.com/VinAIResearch/dict-guided">github</a> /
          <a
            href="https://openaccess.thecvf.com/content/CVPR2021/html/Nguyen_Dictionary-Guided_Scene_Text_Recognition_CVPR_2021_paper.html">paper</a>
          <p></p>
          <p>
            A novel approach to incorporate dictionary on both training and testing phases. Additionally, we
            also introduced a novel Vietnamese scene text dataset (VinText), the largest scene text dataset for
            Vietnamese.
          </p>
        </td>
      </tr>


    </tbody>
  </table>


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr>
        <td>
          <heading>Master Thesis</heading>
        </td>
      </tr>
    </tbody>
  </table>
  <table width="100%" align="center" border="0" cellpadding="20">
    <tbody>
      <tr>
        <td style="padding:20px;width:5%;vertical-align:middle"><img src="images/object_state_understanding.png"
            width="160"></td>
        <td width="75%" valign="center">
          <p style="font-size: 1.5em;"><b>State-aware Object Understanding</b>
            <br>
            <!-- <i>WACV 2022, CVPR 2023, CVPR 2024, ACMMM 2024, AAAI 2025</i> -->
            [<a href="https://www.researchgate.net/publication/387971154_State-Aware_Object_Understanding">Master's
              Thesis</a>]
          </p>
        </td>
      </tr>
  </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr>
        <td>
          <heading>Services</heading>
        </td>
      </tr>
    </tbody>
  </table>
  <table width="100%" align="center" border="0" cellpadding="20">
    <tbody>
      <tr>
        <td style="padding:20px;width:5%;vertical-align:middle"><img src="images/reviewer.jpg"></td>
        <td width="75%" valign="center">
          <p><b>Reviewer</b>
            <br>
            <i>WACV 2022, CVPR 2023, CVPR 2024, ACMMM 2024, AAAI 2025, NAACL 2025, CVPR 2025, ACL 2025</i>
          </p>
        </td>
      </tr>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/ai-challenge.png"></td>
        <td width="75%" valign="center">
          <p><b>Competition jury member (2021)</b>
            <br>
            <i>Ho Chi Minh city AI Challenge 2021: Vietnamese Scene Text Recognition</i>
          </p>
        </td>
      </tr>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/teaching-assistant.png" alt="TA">
        </td>
        <td width="75%" valign="center">
          <p><b>Teaching assistant (2018 - 2019)</b>
            <br>
            <i>University of Engineering and Technology - Vietnam National University</i>
            <br>
            Teaching assistant in several computer vision and machine learning courses for Samsung Display
            Vietnam's staff
          </p>
        </td>
      </tr>
    </tbody>
  </table>


  <!--  Timeline -->
  <span class="anchor" id="resume"></span>
  <div class="docs-section">
    <div class="docs-section">
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
          <tr>
            <td>
              <heading>Short CV</heading>
            </td>
          </tr>
        </tbody>
      </table>
      <ul class="timeline">

        <li>

          <div class="direction-l">

            <div class="flag-wrapper">
              <span class="flag">Aitomatic</span>
              <span class="time-wrapper"><span class="time">July. 2024 - present</span></span>
            </div>
            <div class="desc"><b>Senior Applied Scientist</b> <br />Domain-specific Foundation Models and Agents</div>
          </div>
        </li>

        <li>
          <div class="direction-r">
            <div class="flag-wrapper">
              <span class="flag">University of Rochester</span>
              <span class="time-wrapper"><span class="time">Aug. 2022 - May. 2024</span></span>
            </div>
            <div class="desc"><b>Master Student</b> <br /> Department of Computer Science</div>
          </div>
        </li>


        <li>
          <div class="direction-l">
            <div class="flag-wrapper">
              <span class="flag">VinAI Research</span>
              <span class="time-wrapper"><span class="time">Jan. 2022- June. 2022</span></span>
            </div>
            <div class="desc"><b>AI Research Engineer</b> <br /> R&D Group</div>
          </div>
        </li>

        <li>
          <div class="direction-l">

            <div class="flag-wrapper">
              <span class="flag">VinAI Research</span>
              <span class="time-wrapper"><span class="time">Dec. 2019- Dec. 2021</span></span>
            </div>
            <div class="desc"><b>AI Research Resident</b> <br /> Computer Vision Research Group</div>
          </div>
        </li>

        <li>

          <div class="direction-l">

            <div class="flag-wrapper">
              <span class="flag">Teko Vietnam</span>
              <span class="time-wrapper"><span class="time">April. 2019- Nov. 2019</span></span>
            </div>
            <div class="desc"><b>AI Engineer Intern</b> <br /> Data Science Group</div>
          </div>
        </li>

        <li>

          <div class="direction-r">

            <div class="flag-wrapper">
              <span class="flag">Vietnam National University</span>
              <span class="time-wrapper"><span class="time">2016 - 2020</span></span>
            </div>
            <div class="desc"><b>B.S. Student</b> <br /> Information Technology Department</div>
          </div>
        </li>

      </ul>




      <table
        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p>This website uses source code from <a href="http://jonbarron.info">http://jonbarron.info</a> and <a
                  href="https://www.yapengtian.com">https://www.yapengtian.com</a> </p>
            </td>
          </tr>
        </tbody>
      </table>
      </td>
      </tr>
      </table>
</body>

</html>