<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>William Nguyen | AI Scientist & Domain LLMs</title>
  <meta name="author" content="Nguyen (William) Nguyen">
  <meta name="description" content="William Nguyen builds AI products adopted by global industrial leaders. Creator of SemiKong and Llamarine. 9 papers at CVPR, NAACL, ACM MM, AAAI. 3 US patents.">
  <meta name="keywords" content="Nguyen Nguyen, William Nguyen, AI researcher, applied scientist, domain-specific LLM, SemiKong, Llamarine, ProSEA, NLP, computer vision">

  <!-- Open Graph -->
  <meta property="og:title" content="William Nguyen | AI Scientist & Domain LLMs">
  <meta property="og:description" content="5+ years turning AI research into products adopted by global industrial leaders. Creator of SemiKong and Llamarine. 9 papers, 3 US patents.">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://nguyennm1024.github.io/">
  <meta property="og:image" content="https://nguyennm1024.github.io/images/ManhNguyen_circle.png">

  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@nguyennm1024">
  <meta name="twitter:title" content="William Nguyen | AI Scientist & Domain LLMs">
  <meta name="twitter:description" content="5+ years turning AI research into products adopted by global industrial leaders. Creator of SemiKong and Llamarine.">

  <link rel="canonical" href="https://nguyennm1024.github.io/">
  <link rel="icon" type="image/png" href="images/buffalo_icon.png">
  <link rel="stylesheet" href="stylesheet.css?v=2">
  <link rel="stylesheet" href="timeline.css?v=2">

  <!-- JSON-LD Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Person",
    "name": "Nguyen (William) Nguyen",
    "url": "https://nguyennm1024.github.io/",
    "image": "https://nguyennm1024.github.io/images/ManhNguyen_circle.png",
    "jobTitle": "Senior Applied Scientist",
    "worksFor": {
      "@type": "Organization",
      "name": "Aitomatic",
      "url": "https://www.aitomatic.com"
    },
    "alumniOf": [
      {
        "@type": "CollegeOrUniversity",
        "name": "University of Rochester"
      },
      {
        "@type": "CollegeOrUniversity",
        "name": "University of Engineering and Technology - Vietnam National University"
      }
    ],
    "sameAs": [
      "https://www.linkedin.com/in/nguyennm1024/",
      "https://scholar.google.com/citations?user=kYok1lsAAAAJ&hl=en",
      "https://github.com/nguyennm1024",
      "https://twitter.com/nguyennm1024"
    ]
  }
  </script>
</head>

<body>
  <!-- Skip to content -->
  <a href="#main-content" class="skip-link">Skip to main content</a>

  <!-- Navigation -->
  <nav class="site-nav" role="navigation" aria-label="Main navigation">
    <div class="nav-container">
      <a href="#hero" class="nav-logo">William Nguyen</a>
      <button class="nav-toggle" aria-expanded="false" aria-controls="nav-menu" aria-label="Toggle navigation menu">
        <span class="nav-toggle-bar"></span>
        <span class="nav-toggle-bar"></span>
        <span class="nav-toggle-bar"></span>
      </button>
      <ul id="nav-menu" class="nav-links" role="menubar">
        <li role="none"><a href="#about" role="menuitem">About</a></li>
        <li role="none"><a href="#news" role="menuitem">News</a></li>
        <li role="none"><a href="#featured-work" role="menuitem">Featured Work</a></li>
        <li role="none"><a href="#publications" role="menuitem">Publications</a></li>
        <li role="none"><a href="#patents" role="menuitem">Patents</a></li>
        <li role="none"><a href="#experience" role="menuitem">Experience</a></li>
        <li role="none"><a href="#services" role="menuitem">Academic Service</a></li>
      </ul>
    </div>
  </nav>

  <!-- Main Content -->
  <main id="main-content">

    <!-- Hero Section -->
    <section id="hero" class="section hero-section">
      <div class="container hero-layout">
        <div class="hero-text">
          <h1 class="hero-name">Nguyen (William) Nguyen</h1>
          <p class="hero-tagline">5+ years turning AI research into products adopted by global industrial leaders</p>
          <p class="hero-title">Senior Applied Scientist at <a href="https://www.aitomatic.com" target="_blank" rel="noopener noreferrer">Aitomatic</a></p>
          <div class="hero-contact-links">
            <a href="mailto:nguyennm1024@gmail.com">Email</a>
            <a href="data/CV_Nguyen-2.pdf">CV</a>
            <a href="https://www.linkedin.com/in/nguyennm1024/" target="_blank" rel="noopener noreferrer">LinkedIn</a>
            <a href="data/Nguyen-bio.txt">Bio</a>
            <a href="https://scholar.google.com/citations?user=kYok1lsAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Google Scholar</a>
            <a href="https://twitter.com/nguyennm1024" target="_blank" rel="noopener noreferrer">Twitter</a>
            <a href="https://github.com/nguyennm1024" target="_blank" rel="noopener noreferrer">Github</a>
          </div>
          <a href="https://chatgpt.com/g/g-67d35695596481919e9deb894d707a24-william-nguyen" class="twin-ai-button" target="_blank" rel="noopener noreferrer">Chat with my AI Twin</a>
        </div>
        <div class="hero-photo">
          <a href="images/ManhNguyen.png">
            <img src="images/ManhNguyen_circle.png" alt="Portrait photo of Nguyen (William) Nguyen" class="profile-photo">
          </a>
        </div>
      </div>
    </section>

    <!-- Highlights Bar -->
    <section class="highlights-section" aria-label="Key achievements">
      <div class="container">
        <div class="highlights-bar">
          <div class="highlight-stat">
            <span class="highlight-number">5+</span>
            <span class="highlight-label">Years in AI</span>
          </div>
          <div class="highlight-stat">
            <span class="highlight-number">9</span>
            <span class="highlight-label">Publications at Top Venues</span>
          </div>
          <div class="highlight-stat">
            <span class="highlight-number">3</span>
            <span class="highlight-label">US Patent Applications</span>
          </div>
          <div class="highlight-stat">
            <span class="highlight-number">800+</span>
            <span class="highlight-label">GitHub Stars</span>
          </div>
        </div>
      </div>
    </section>

    <!-- About Section -->
    <section id="about" class="section about-section">
      <div class="container">
        <h2 class="section-heading">About</h2>
        <div class="about-content">
          <p>I build domain-specific AI systems that solve real problems in industries where general-purpose models fall short. At <a href="https://www.aitomatic.com" target="_blank" rel="noopener noreferrer">Aitomatic</a>, I led development of <a href="https://arxiv.org/abs/2411.13802" target="_blank" rel="noopener noreferrer">SemiKong</a> and <a href="https://arxiv.org/abs/2503.00203" target="_blank" rel="noopener noreferrer">Llamarine</a>, the first open-source LLMs for the semiconductor and maritime industries, now adopted by global industrial leaders including Tokyo Electron and Furuno. My multi-agent framework ProSEA achieved 93.2% accuracy on FinanceBench, outperforming established frameworks like LlamaIndex RAG and LangChain ReAct.</p>
          <p>Before Aitomatic, I earned my master's degree from the University of Rochester (advisor: Prof. <a href="https://www.cs.rochester.edu/~cxu22/" target="_blank" rel="noopener noreferrer">Chenliang Xu</a>), where my work on object state understanding achieved on par with the commercial SOTA model with a smaller, faster model. I spent nearly 3 years at <a href="https://www.vinai.io/" target="_blank" rel="noopener noreferrer">VinAI Research</a> working under Professor <a href="https://www3.cs.stonybrook.edu/~minhhoai/" target="_blank" rel="noopener noreferrer">Nguyen Minh Hoai</a>, where I surpassed state-of-the-art on scene text recognition by 3-5% and reduced annotation costs by 50%. I completed my bachelor's with Distinction from <a href="https://e.uet.vnu.edu.vn/" target="_blank" rel="noopener noreferrer">Vietnam National University</a>, where I worked with Professor <a href="https://sites.google.com/site/xiemhoang/" target="_blank" rel="noopener noreferrer">Hoang Van Xiem</a>.</p>
          <p>My research spans NLP, computer vision, and machine learning, published at CVPR, NAACL, ACM MM, and AAAI. I currently focus on intelligent AI agents that learn from failure to self-improve, and on specialized LLMs for industries like semiconductors and maritime. SemiKong was featured by VentureBeat, Meta AI Blog, Tom's Hardware, and shared by Yann LeCun.</p>
        </div>
      </div>
    </section>

    <!-- News Section -->
    <section id="news" class="section news-section">
      <div class="container">
        <h2 class="section-heading">News</h2>
        <div class="news-scroll-box">
          <ul class="news-list" aria-label="News and updates">
            <li class="news-item"><span class="news-date">03/2025</span> One paper accepted at JSAI 2025.</li>
            <li class="news-item"><span class="news-date">12/2024</span> One paper accepted at OSAI4MU-25 Workshop, AAAI 2025.</li>
            <li class="news-item"><span class="news-date">07/2024</span> Our SemiKong work was featured in <a href="https://venturebeat.com/ai/aitomatics-semikong-uses-ai-to-reshape-chipmaking-processes/" target="_blank" rel="noopener noreferrer">VentureBeat</a>, <a href="https://www.msn.com/en-us/money/other/semikong-is-the-worlds-first-open-source-semiconductor-focused-llm-it-claims-to-bring-new-chips-to-market-30-faster/ar-AA1wD6nS" target="_blank" rel="noopener noreferrer">MSN</a>, <a href="https://www.linkedin.com/posts/yann-lecun_semikong-by-aitomatic-built-with-llama-to-activity-7270625478237945856-cTex/" target="_blank" rel="noopener noreferrer">Yann LeCun's share</a>, <a href="https://ai.meta.com/blog/aitomatic-built-with-llama/" target="_blank" rel="noopener noreferrer">Meta AI Blog</a>, <a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/semikong-is-the-worlds-first-open-source-semiconductor-focused-llm-it-claims-to-bring-new-chips-to-market-30-percent-faster" target="_blank" rel="noopener noreferrer">Tom's Hardware</a>, <a href="https://www.marktechpost.com/2024/12/27/meet-semikong-the-worlds-first-open-source-semiconductor-focused-llm/" target="_blank" rel="noopener noreferrer">MarkTechPost</a>, <a href="https://digialps.com/semikong-an-open-source-semiconductor-centric-llm-powered-by-meta-llama-3-1/" target="_blank" rel="noopener noreferrer">Digialps</a>, <a href="https://www.gadgets360.com/ai/news/aitomatic-ai-alliance-semikong-model-semiconductor-focused-open-source-meta-7363931" target="_blank" rel="noopener noreferrer">Gadgets360</a>, and many other media.</li>
            <li class="news-item"><span class="news-date">07/2024</span> One paper accepted at ACM MM 2024.</li>
            <li class="news-item"><span class="news-date">07/2024</span> I joined Aitomatic as a Senior Applied Scientist.</li>
            <li class="news-item"><span class="news-date">03/2024</span> One paper accepted at NAACL 2024.</li>
            <li class="news-item"><span class="news-date">03/2024</span> I received research internship offers from Bosch AI Research and Amazon, USA.</li>
            <li class="news-item"><span class="news-date">07/2023</span> One paper accepted at AV4D Workshop, ICCV 2023.</li>
            <li class="news-item"><span class="news-date">08/2022</span> I started my journey with the University of Rochester since the Fall 2022.</li>
            <li class="news-item"><span class="news-date">01/2022</span> I began working as an AI Research Engineer with the applied team at VinAI Research.</li>
            <li class="news-item"><span class="news-date">03/2021</span> One paper accepted at CVPR 2021.</li>
            <li class="news-item"><span class="news-date">07/2020</span> I graduated with Distinction from Vietnam National University in 2020.</li>
            <li class="news-item"><span class="news-date">12/2019</span> I started my journey with VinAI Research as an AI Research Resident in December 2019.</li>
          </ul>
        </div>
      </div>
    </section>

    <!-- Featured Work Section -->
    <section id="featured-work" class="section featured-section">
      <div class="container">
        <h2 class="section-heading">Featured Work</h2>
        <p class="section-intro">Domain-specific AI systems that bring expert-level intelligence to industries where general-purpose models fall short.</p>
        <div class="featured-grid">

          <article class="featured-card" data-project="prosea">
            <div class="featured-card-image">
              <a href="projects/prosea.html"><img src="images/prosea.png" alt="ProSEA framework architecture diagram" loading="lazy"></a>
            </div>
            <div class="featured-card-content">
              <h3 class="paper-title"><a href="projects/prosea.html">ProSEA: Problem Solving via Exploration Agents</a></h3>
              <p class="paper-venue"><em>preprint</em>, 2025</p>
              <div class="paper-links">
                <a href="https://arxiv.org/abs/2510.07423" target="_blank" rel="noopener noreferrer">paper</a>
              </div>
              <p class="paper-description">93.2% accuracy on FinanceBench, outperforming LlamaIndex RAG (56.7%), LangChain ReAct (81.6%), and OpenAI Assistants (42.7%). Learns from failures to adaptively replan.</p>
            </div>
          </article>

          <article class="featured-card" data-project="llamarine">
            <div class="featured-card-image">
              <a href="projects/llamarine.html"><img src="images/llamarine.png" alt="Llamarine maritime LLM overview" loading="lazy"></a>
            </div>
            <div class="featured-card-content">
              <h3 class="paper-title"><a href="projects/llamarine.html">Llamarine: Maritime Industry-specific LLM</a></h3>
              <p class="paper-venue"><em>JSAI</em>, 2025</p>
              <div class="paper-links">
                <a href="https://arxiv.org/abs/2503.00203" target="_blank" rel="noopener noreferrer">paper</a>
                <a href="https://huggingface.co/pentagoniac/llamarine" target="_blank" rel="noopener noreferrer">model</a>
              </div>
              <p class="paper-description">The first open-source maritime LLM. Adopted by Furuno for a navigation assistant achieving 100% accuracy on regulation-compliant actions.</p>
            </div>
          </article>

          <article class="featured-card" data-project="semikong">
            <div class="featured-card-image">
              <a href="projects/semikong.html"><img src="images/semikong.png" alt="SemiKong semiconductor LLM architecture" loading="lazy"></a>
            </div>
            <div class="featured-card-content">
              <h3 class="paper-title"><a href="projects/semikong.html">SemiKong: Semiconductor Industry-Specific LLM</a></h3>
              <p class="paper-venue"><em>OSAI4MU-25 Workshop, AAAI</em>, 2025</p>
              <div class="paper-links">
                <a href="https://arxiv.org/abs/2411.13802" target="_blank" rel="noopener noreferrer">paper</a>
                <a href="https://github.com/aitomatic/semikong" target="_blank" rel="noopener noreferrer">github</a>
              </div>
              <p class="paper-description">The first open-source semiconductor LLM. Adopted by Tokyo Electron for root cause analysis, reducing troubleshooting time by 30%. Featured by VentureBeat and shared by Yann LeCun.</p>
            </div>
          </article>

        </div>
      </div>
    </section>

    <!-- Publications Section -->
    <section id="publications" class="section publications-section">
      <div class="container">
        <h2 class="section-heading">Publications &amp; Research</h2>

        <div class="publications-list">

          <article class="pub-card" data-year="2025">
            <div class="pub-card-image">
              <img src="images/prosea.png" alt="ProSEA framework architecture diagram" loading="lazy">
            </div>
            <div class="pub-card-content">
              <h3 class="paper-title">ProSEA: Problem Solving via Exploration Agents</h3>
              <p class="paper-authors">
                <strong>William Nguyen</strong>,
                <a href="https://www.linkedin.com/in/luongthevinh/" target="_blank" rel="noopener noreferrer">Vinh Luong</a>,
                <a href="https://scholar.google.com/citations?user=3KvkkfoAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Christopher Nguyen</a>
              </p>
              <p class="paper-venue"><em>preprint</em>, 2025</p>
              <div class="paper-links">
                <a href="https://arxiv.org/abs/2510.07423" target="_blank" rel="noopener noreferrer">paper</a>
              </div>
              <p class="paper-description">Complex real-world problems require more than a single AI call; they demand iterative reasoning. ProSEA is a modular multi-agent framework where a Manager Agent orchestrates domain-specialized Expert Agents, enabling adaptive replanning based on structured feedback from failures and newly discovered constraints. Achieves state-of-the-art performance on the FinanceBench benchmark.</p>
            </div>
          </article>

          <article class="pub-card" data-year="2025">
            <div class="pub-card-image">
              <img src="images/llamarine.png" alt="Llamarine maritime LLM overview" loading="lazy">
            </div>
            <div class="pub-card-content">
              <h3 class="paper-title">Llamarine: Open-source Maritime Industry-specific Large Language Model</h3>
              <p class="paper-authors">
                <strong>William Nguyen</strong>,
                An Phan,
                Konobu Kimura,
                Hitoshi Maeno,
                Mika Tanaka,
                Quynh Le,
                William Poucher,
                <a href="https://scholar.google.com/citations?user=3KvkkfoAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Christopher Nguyen</a>
              </p>
              <p class="paper-venue"><em>Annual Conference of the Japanese Society for Artificial Intelligence</em>, 2025</p>
              <div class="paper-links">
                <a href="https://arxiv.org/abs/2503.00203" target="_blank" rel="noopener noreferrer">paper</a>
                <a href="https://huggingface.co/pentagoniac/llamarine" target="_blank" rel="noopener noreferrer">model</a>
              </div>
              <p class="paper-description">The first open-source domain-specific LLM for the maritime industry. Outperforms several commercial products including GPT-4o-mini, GPT-4o, Claude-3.5-Sonnet, and open-source models such as Llama3.1 8B, Llama3.1 70B, and Llama3.3 70B.</p>
            </div>
          </article>

          <article class="pub-card" data-year="2025">
            <div class="pub-card-image">
              <img src="images/semikong.png" alt="SemiKong semiconductor LLM architecture" loading="lazy">
            </div>
            <div class="pub-card-content">
              <h3 class="paper-title">SemiKong: Curating, Training, and Evaluating A Semiconductor Industry-Specific Large Language Model</h3>
              <p class="paper-authors">
                <a href="https://scholar.google.com/citations?user=3KvkkfoAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Christopher Nguyen</a>,
                <strong>William Nguyen</strong>,
                Atsushi Suzuki,
                Daisuke Oku,
                Hong An Phan,
                Sang Dinh,
                Zooey Nguyen,
                Anh Hai Ha,
                <a href="https://www.linkedin.com/in/shrutiraghavan/" target="_blank" rel="noopener noreferrer">Shruti Raghavan</a>,
                Huy Vo,
                Thang Nguyen,
                Lan Nguyen,
                Yoshikuni Hirayama
              </p>
              <p class="paper-venue"><em>OSAI4MU-25 Workshop, AAAI</em>, 2025</p>
              <div class="paper-links">
                <a href="https://arxiv.org/abs/2411.13802" target="_blank" rel="noopener noreferrer">paper</a>
                <a href="https://github.com/aitomatic/semikong" target="_blank" rel="noopener noreferrer">github</a>
              </div>
              <p class="paper-description">The first open-source semiconductor-focused LLM. Outperforms several commercial products including Claude-3.5-Sonnet, Haiku, Opus, Command-R, and open-source models such as Llama3 70B. Featured by VentureBeat, Meta AI Blog, Tom's Hardware, and shared by Yann LeCun.</p>
            </div>
          </article>

          <article class="pub-card" data-year="2024">
            <div class="pub-card-image">
              <img src="images/dana.png" alt="DANA neurosymbolic agent framework diagram" loading="lazy">
            </div>
            <div class="pub-card-content">
              <h3 class="paper-title">DANA: Domain-Aware Neurosymbolic Agents for Consistency and Accuracy</h3>
              <p class="paper-authors">
                <a href="https://www.linkedin.com/in/luongthevinh/" target="_blank" rel="noopener noreferrer">Vinh Luong</a>,
                Sang Dinh,
                <a href="https://www.linkedin.com/in/shrutiraghavan/" target="_blank" rel="noopener noreferrer">Shruti Raghavan</a>,
                <strong>William Nguyen</strong>,
                Zooey Nguyen,
                Quynh Le,
                Hung Vo,
                Kentaro Maegaito,
                Loc Nguyen,
                Thao Nguyen,
                Anh Hai Ha,
                <a href="https://scholar.google.com/citations?user=3KvkkfoAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Christopher Nguyen</a>
              </p>
              <p class="paper-venue"><em>preprint</em>, 2024</p>
              <div class="paper-links">
                <a href="https://thealliance.ai/blog/domain-aware-neurosymbolic-agent-dana-architecture" target="_blank" rel="noopener noreferrer">project page</a>
                <a href="https://github.com/aitomatic/openssa/" target="_blank" rel="noopener noreferrer">github</a>
                <a href="https://arxiv.org/abs/2410.02823" target="_blank" rel="noopener noreferrer">paper</a>
              </div>
              <p class="paper-description">A unified agentic framework that incorporates expert knowledge using neurosymbolic methods to create domain-specific agents with high consistency and accuracy, significantly outperforming ChatGPT assistant and ReAct agent.</p>
            </div>
          </article>

          <article class="pub-card" data-year="2024">
            <div class="pub-card-image">
              <img src="images/eagle.gif" alt="EAGLE egocentric video understanding model" loading="lazy">
            </div>
            <div class="pub-card-content">
              <h3 class="paper-title">EAGLE: Egocentric AGgregated Language-video Engine</h3>
              <p class="paper-authors">
                <a href="https://jing-bi.github.io" target="_blank" rel="noopener noreferrer">Jing Bi</a>,
                <a href="https://yunlong10.github.io" target="_blank" rel="noopener noreferrer">Yunlong Tang</a>,
                <a href="https://songluchuan.github.io" target="_blank" rel="noopener noreferrer">Luchuan Song</a>,
                <a href="https://alivosoughi.com" target="_blank" rel="noopener noreferrer">Ali Vosoughi</a>,
                <strong>Nguyen Nguyen</strong>,
                <a href="https://www.cs.rochester.edu/~cxu22/" target="_blank" rel="noopener noreferrer">Chenliang Xu</a>
              </p>
              <p class="paper-venue"><em>ACM MM</em>, 2024</p>
              <div class="paper-links">
                <a href="https://dl.acm.org/doi/10.1145/3664647.3681618" target="_blank" rel="noopener noreferrer">paper</a>
              </div>
              <p class="paper-description">A unified multimodal LLM designed to comprehensively solve tasks related to egocentric video understanding, enabling AI to interpret first-person video from the viewer's perspective.</p>
            </div>
          </article>

          <article class="pub-card" data-year="2024">
            <div class="pub-card-image">
              <img src="images/oscar.png" alt="OSCaR object state captioning illustration" loading="lazy">
            </div>
            <div class="pub-card-content">
              <h3 class="paper-title">OSCaR: Object State Captioning and State Change Representation</h3>
              <p class="paper-authors">
                <strong>Nguyen Nguyen</strong>,
                <a href="https://jing-bi.github.io" target="_blank" rel="noopener noreferrer">Jing Bi</a>,
                <a href="https://alivosoughi.com" target="_blank" rel="noopener noreferrer">Ali Vosoughi</a>,
                <a href="https://www.yapengtian.com" target="_blank" rel="noopener noreferrer">Yapeng Tian</a>,
                <a href="https://pooyanfazli.com" target="_blank" rel="noopener noreferrer">Pooyan Fazli</a>,
                <a href="https://www.cs.rochester.edu/~cxu22/" target="_blank" rel="noopener noreferrer">Chenliang Xu</a>
              </p>
              <p class="paper-venue"><em>NAACL</em>, 2024</p>
              <div class="paper-links">
                <a href="https://github.com/nguyennm1024/OSCaR" target="_blank" rel="noopener noreferrer">github</a>
                <a href="https://arxiv.org/abs/2402.17128" target="_blank" rel="noopener noreferrer">paper</a>
              </div>
              <p class="paper-description">Introduces a new task for understanding object states and how they change over time. The trained Multimodal-LLM significantly surpasses previous state-of-the-art models and achieves on par with the commercial SOTA model on both GPT-4 and human evaluations.</p>
            </div>
          </article>

          <article class="pub-card" data-year="2024">
            <div class="pub-card-image">
              <img src="images/linguistics_detection.png" alt="Linguistics-based scene text spotting approach" loading="lazy">
            </div>
            <div class="pub-card-content">
              <h3 class="paper-title">Efficiently Leveraging Linguistics Knowledge for Scene Text Spotting</h3>
              <p class="paper-authors">
                <strong>Nguyen Nguyen</strong>,
                <a href="https://www.yapengtian.com" target="_blank" rel="noopener noreferrer">Yapeng Tian</a>,
                <a href="https://www.cs.rochester.edu/~cxu22/" target="_blank" rel="noopener noreferrer">Chenliang Xu</a>
              </p>
              <p class="paper-venue"><em>Arxiv</em></p>
              <div class="paper-links">
                <a href="https://arxiv.org/abs/2402.17134" target="_blank" rel="noopener noreferrer">paper</a>
              </div>
              <p class="paper-description">A simple but effective approach that incorporates language knowledge from large text corpora to improve both text detection and recognition in natural scenes.</p>
            </div>
          </article>

          <article class="pub-card" data-year="2023">
            <div class="pub-card-image">
              <img src="images/misar.jpeg" alt="MISAR multimodal instructional system with AR" loading="lazy">
            </div>
            <div class="pub-card-content">
              <h3 class="paper-title">MISAR: A Multimodal Instructional System with Augmented Reality</h3>
              <p class="paper-authors">
                <a href="https://jing-bi.github.io" target="_blank" rel="noopener noreferrer">Jing Bi</a>*,
                <strong>Nguyen Nguyen*</strong>,
                <a href="https://alivosoughi.com" target="_blank" rel="noopener noreferrer">Ali Vosoughi</a>*,
                <a href="https://www.cs.rochester.edu/~cxu22/" target="_blank" rel="noopener noreferrer">Chenliang Xu</a>
                (* equal contribution)
              </p>
              <p class="paper-venue"><em>AV4D Workshop, ICCV</em>, 2023</p>
              <div class="paper-links">
                <a href="https://github.com/nguyennm1024/misar" target="_blank" rel="noopener noreferrer">github</a>
                <a href="https://av4d.org/papers/iccv23/p12.pdf" target="_blank" rel="noopener noreferrer">paper</a>
              </div>
              <p class="paper-description">A comprehensive system that guides humans to work more efficiently and accurately by leveraging LLMs to interpret and process information from visual, auditory, and contextual dimensions.</p>
            </div>
          </article>

          <article class="pub-card" data-year="2021">
            <div class="pub-card-image">
              <div class="pub-image-hover">
                <img src="images/st_before.png" alt="Dictionary-guided scene text recognition before" loading="lazy">
                <img src="images/st_after.png" alt="Dictionary-guided scene text recognition after" class="pub-image-overlay" loading="lazy">
              </div>
            </div>
            <div class="pub-card-content">
              <h3 class="paper-title">
                <a href="https://www.vinai.io/publication-posts/dictionary-guided-scene-text-recognition" target="_blank" rel="noopener noreferrer">Dictionary-guided Scene Text Recognition</a>
              </h3>
              <p class="paper-authors">
                <strong>Nguyen Nguyen</strong>,
                Thu Nguyen,
                <a href="https://www3.cs.stonybrook.edu/~tquangvinh/" target="_blank" rel="noopener noreferrer">Vinh Tran</a>,
                <a href="https://www.fit.hcmus.edu.vn/~tmtriet/" target="_blank" rel="noopener noreferrer">Minh Triet Tran</a>,
                <a href="https://scholar.google.com/citations?user=I8bNZakAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Thanh Duc Ngo</a>,
                <a href="https://ix.cs.uoregon.edu/~thien/" target="_blank" rel="noopener noreferrer">Thien Huu Nguyen</a>,
                <a href="https://www3.cs.stonybrook.edu/~minhhoai/" target="_blank" rel="noopener noreferrer">Minh Hoai Nguyen</a>
              </p>
              <p class="paper-venue"><em>CVPR</em>, 2021</p>
              <div class="paper-links">
                <a href="https://www.vinai.io/publication-posts/dictionary-guided-scene-text-recognition" target="_blank" rel="noopener noreferrer">project page</a>
                <a href="https://github.com/VinAIResearch/dict-guided" target="_blank" rel="noopener noreferrer">github</a>
                <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Nguyen_Dictionary-Guided_Scene_Text_Recognition_CVPR_2021_paper.html" target="_blank" rel="noopener noreferrer">paper</a>
              </div>
              <p class="paper-description">A novel approach to incorporate dictionary knowledge in both training and testing phases for scene text recognition. Also introduces VinText, the largest scene text dataset for Vietnamese.</p>
            </div>
          </article>

        </div>

        <!-- Master Thesis -->
        <div class="thesis-section">
          <h3 class="subsection-heading">Master Thesis</h3>
          <article class="pub-card" data-year="2024">
            <div class="pub-card-image">
              <img src="images/object_state_understanding.png" alt="State-aware object understanding thesis illustration" loading="lazy">
            </div>
            <div class="pub-card-content">
              <h3 class="paper-title">State-aware Object Understanding</h3>
              <p class="paper-authors">
                <strong>Nguyen Nguyen</strong>
              </p>
              <p class="paper-venue"><em>University of Rochester, Department of Computer Science</em>, 2024</p>
              <div class="paper-links">
                <a href="https://www.researchgate.net/publication/387971154_State-Aware_Object_Understanding" target="_blank" rel="noopener noreferrer">thesis</a>
              </div>
              <p class="paper-description">Can AI truly understand what happens when you crack an egg or fold a shirt? This thesis tackles a fundamental challenge in vision-language intelligence: teaching models to perceive, describe, and reason about how objects change state. The resulting system achieves on par with the commercial SOTA model using a smaller, faster architecture â€” pointing toward AI that understands not just what things are, but what happens to them.</p>
            </div>
          </article>
        </div>

      </div>
    </section>

    <!-- Patents Section -->
    <section id="patents" class="section patents-section">
      <div class="container">
        <h2 class="section-heading">Patents</h2>
        <div class="patents-list">
          <article class="patent-card">
            <h3 class="paper-title">Delivering Domain-Expert Agents and Models Using Synthetic Knowledge</h3>
            <p class="paper-authors">Christopher Nguyen, <strong>Manh-Nguyen Nguyen</strong>, Hong An Phan, Zooey Nhu-Quynh Nguyen, The-Vinh Luong, Elise NhuY Nguyen, Thomas Rasmussen, Anh Hai Ha, Phi-Hung Vo, Xuan-Sang Dinh, Huy-Thuan Bui, Anh-Quoc Dang, Timothy Michael Gerard Rozario</p>
            <p class="paper-venue">US Patent App. 63/726,322, 2024</p>
          </article>
          <article class="patent-card">
            <h3 class="paper-title">Delivering Domain-Expert Agents for Improving Problem-Solving</h3>
            <p class="paper-authors">Christopher Nguyen, <strong>Manh-Nguyen Nguyen</strong>, Hong An Phan, Zooey Nhu-Quynh Nguyen, The-Vinh Luong, Elise Nhu-Y Nguyen, Thomas Rasmussen, Anh Hai Ha, Phi-Hung Vo, Xuan-Sang Dinh, Huy-Thuan Bui, Anh-Quoc Dang</p>
            <p class="paper-venue">US Patent App. 63/721,419, 2024</p>
          </article>
          <article class="patent-card">
            <h3 class="paper-title">Domain-Aware Neurosymbolic Agents For Improving Problem-Solving Accuracy And Consistency</h3>
            <p class="paper-authors">Christopher Nguyen, The Vinh Luong, Xuan Sang Dinh, Zooey Nhu-Quynh Nguyen, Shruti Raghavan, <strong>Manh-Nguyen Nguyen</strong>, Quynh Thi-Tham Le, Phi Hung Vo, Tan Loc Nguyen, Anh Hai Ha, Phuong Thao Nguyen</p>
            <p class="paper-venue">US Patent App. 63/696,337, 2024</p>
          </article>
        </div>
      </div>
    </section>

    <!-- Experience / Timeline Section -->
    <section id="experience" class="section experience-section">
      <div class="container">
        <h2 class="section-heading">Experience</h2>
        <ul class="timeline">
          <li>
            <div class="direction-l">
              <div class="flag-wrapper">
                <span class="flag">Aitomatic</span>
                <span class="time-wrapper"><span class="time">July. 2024 - present</span></span>
              </div>
              <div class="desc"><b>Senior Applied Scientist</b><br>Domain-specific Foundation Models and Agents</div>
            </div>
          </li>
          <li>
            <div class="direction-r">
              <div class="flag-wrapper">
                <span class="flag">University of Rochester</span>
                <span class="time-wrapper"><span class="time">Aug. 2022 - May. 2024</span></span>
              </div>
              <div class="desc"><b>Research Assistant</b><br>Department of Computer Science</div>
            </div>
          </li>
          <li>
            <div class="direction-l">
              <div class="flag-wrapper">
                <span class="flag">VinAI Research</span>
                <span class="time-wrapper"><span class="time">Jan. 2022- June. 2022</span></span>
              </div>
              <div class="desc"><b>AI Research Engineer</b><br>R&amp;D Group</div>
            </div>
          </li>
          <li>
            <div class="direction-l">
              <div class="flag-wrapper">
                <span class="flag">VinAI Research</span>
                <span class="time-wrapper"><span class="time">Dec. 2019- Dec. 2021</span></span>
              </div>
              <div class="desc"><b>AI Research Resident</b><br>Computer Vision Research Group</div>
            </div>
          </li>
          <li>
            <div class="direction-l">
              <div class="flag-wrapper">
                <span class="flag">Teko Vietnam</span>
                <span class="time-wrapper"><span class="time">April. 2019- Nov. 2019</span></span>
              </div>
              <div class="desc"><b>AI Engineer Intern</b><br>Data Science Group</div>
            </div>
          </li>
          <li>
            <div class="direction-r">
              <div class="flag-wrapper">
                <span class="flag">Vietnam National University</span>
                <span class="time-wrapper"><span class="time">2016 - 2020</span></span>
              </div>
              <div class="desc"><b>B.S. Student</b><br>Information Technology Department</div>
            </div>
          </li>
        </ul>
      </div>
    </section>

    <!-- Services Section -->
    <section id="services" class="section services-section">
      <div class="container">
        <h2 class="section-heading">Academic Service</h2>
        <div class="services-list">

          <article class="service-item">
            <div class="service-image">
              <img src="images/reviewer.jpg" alt="Conference reviewer icon" loading="lazy">
            </div>
            <div class="service-content">
              <h3>Reviewer</h3>
              <p><em>WACV 2022, CVPR 2023, CVPR 2024, ACMMM 2024, AAAI 2025, NAACL 2025, CVPR 2025, ACL 2025</em></p>
            </div>
          </article>

          <article class="service-item">
            <div class="service-image">
              <img src="images/reviewer.jpg" alt="Invited speaker icon" loading="lazy">
            </div>
            <div class="service-content">
              <h3>Invited Speaker</h3>
              <p><em>VinAI 2021, OSAI4MU AAAI'25</em></p>
            </div>
          </article>

          <article class="service-item">
            <div class="service-image">
              <img src="images/ai-challenge.png" alt="Ho Chi Minh city AI Challenge logo" loading="lazy">
            </div>
            <div class="service-content">
              <h3>Organizer &amp; Competition jury member (2021)</h3>
              <p><em>Vietnamese Scene Text Challenge 2021 / Ho Chi Minh city AI Challenge 2021: Vietnamese Scene Text Recognition</em></p>
            </div>
          </article>

          <article class="service-item">
            <div class="service-image">
              <img src="images/teaching-assistant.png" alt="Teaching assistant icon" loading="lazy">
            </div>
            <div class="service-content">
              <h3>Teaching assistant (2018 - 2019)</h3>
              <p><em>Computer vision and machine learning courses for Samsung Display Vietnam's staff, University of Engineering and Technology - Vietnam National University</em></p>
            </div>
          </article>

        </div>
      </div>
    </section>

  </main>

  <!-- Footer -->
  <footer class="site-footer">
    <div class="container">
      <p>This website uses source code from <a href="https://jonbarron.info" target="_blank" rel="noopener noreferrer">jonbarron.info</a> and <a href="https://www.yapengtian.com" target="_blank" rel="noopener noreferrer">yapengtian.com</a></p>
    </div>
  </footer>

  <!-- JavaScript -->
  <script>
  (function() {
    // Mobile nav toggle
    var toggle = document.querySelector('.nav-toggle');
    var menu = document.getElementById('nav-menu');
    toggle.addEventListener('click', function() {
      var expanded = toggle.getAttribute('aria-expanded') === 'true';
      toggle.setAttribute('aria-expanded', !expanded);
      menu.classList.toggle('nav-open');
    });
    // Close nav on link click (mobile)
    menu.querySelectorAll('a').forEach(function(link) {
      link.addEventListener('click', function() {
        toggle.setAttribute('aria-expanded', 'false');
        menu.classList.remove('nav-open');
      });
    });


  })();
  </script>
</body>

</html>
